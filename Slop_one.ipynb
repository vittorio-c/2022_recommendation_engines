{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Get data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# df_movies = pd.read_csv('../data_2/lens-movies.csv')\n",
    "df_movies = pd.read_csv('../data_big/movies.csv')\n",
    "# df_rating = pd.read_csv('../data_2/lens-ratings.csv')\n",
    "df_rating = pd.read_csv('../data_big/ratings.csv')\n",
    "df_rating = df_rating[df_rating['movieId'].between(1, 1000)]\n",
    "df_rating = df_rating.drop('timestamp', axis=1)\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_rating[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non weighted\n",
    "\n",
    "For the non weighted version of the algorithm, we can simply re-use the unmodified one from package surprise, which does all the job for us."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from surprise import SlopeOne\n",
    "\n",
    "slop_one = SlopeOne()\n",
    "slop_one.fit(trainset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(slop_one, open('serialized/slop_one_regular.pkl', 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weighted\n",
    "\n",
    "For the weighted version, we will copy-paste the code from surprise package, and tweak a little bit the calculation in the `estimate` function, in order to add the weight.\n",
    "\n",
    "See 'Mathematics' section for details about the calculation.\n",
    "\n",
    "Then, we will compare the results from both versions of the algorithm."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from surprise import AlgoBase\n",
    "from surprise.prediction_algorithms import PredictionImpossible\n",
    "\n",
    "class WeightedSlopOne(AlgoBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.user_mean = None\n",
    "        self.dev = None\n",
    "        self.freq = None\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown.')\n",
    "\n",
    "        ratings = [(i_id, i_r) for (i_id, i_r) in self.trainset.ur[u] if self.freq[i, i_id] > 0]\n",
    "\n",
    "        # ratings = the items rated by the user u, that are also\n",
    "        # rated by at least another user at the same time.\n",
    "        # If ratings is empty, we simply return the mean of\n",
    "        # the ratings of the user as a (poor) estimation.\n",
    "        est = self.user_mean[u]\n",
    "        if ratings:\n",
    "            est = sum((self.dev[i, i_id] + i_r) * self.freq[i, i_id] for (i_id, i_r) in ratings) / \\\n",
    "                  sum(self.freq[i, i_id] for (i_id, _) in ratings)\n",
    "\n",
    "        return est\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        # This will put trainset in self.transet of AlgoBase\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        n_items = trainset.n_items\n",
    "\n",
    "        freq = np.zeros((trainset.n_items, trainset.n_items), int)\n",
    "        dev = np.zeros((trainset.n_items, trainset.n_items), np.double)\n",
    "\n",
    "        # Computation of freq and dev arrays.\n",
    "        # NB These two loops will be much slower than the one from surpise package,\n",
    "        # because they do not use c-python code.\n",
    "        for u, u_ratings in trainset.ur.items():\n",
    "            for i, r_ui in u_ratings:\n",
    "                for j, r_uj in u_ratings:\n",
    "                    freq[i, j] += 1\n",
    "                    dev[i, j] += r_ui - r_uj\n",
    "\n",
    "        for i in range(n_items):\n",
    "            dev[i, i] = 0\n",
    "            for j in range(i + 1, n_items):\n",
    "                dev[i, j] /= freq[i, j]\n",
    "                dev[j, i] = -dev[i, j]\n",
    "\n",
    "        self.freq = freq\n",
    "        self.dev = dev\n",
    "\n",
    "        # mean ratings of all users: mu_u\n",
    "        self.user_mean = [np.mean([r for (_, r) in trainset.ur[u]])\n",
    "                          for u in trainset.all_users()]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def one_user(self, uid, item_ids):\n",
    "        predictions = [self.predict(uid, iid) for iid in item_ids]\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model (much longer due to the fact that we don't use c-python code)\n",
    "weighted_slop_one = WeightedSlopOne()\n",
    "\n",
    "weighted_slop_one.fit(trainset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(weighted_slop_one, open('serialized/slop_one_1003.pkl', 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparison\n",
    "\n",
    "By comparing both models, we see that weighting our ratings improve the predictions by roughly 1%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from surprise import accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_base = slop_one.test(testset)\n",
    "rmse_base = accuracy.rmse(predictions_base)\n",
    "\n",
    "rmse_base"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_weighted = weighted_slop_one.test(testset)\n",
    "rmse_weighted = accuracy.rmse(predictions_weighted)\n",
    "\n",
    "rmse_weighted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross-validation\n",
    "\n",
    "In order to get a better idea on how well the model will perform, we can cross-validate 5 times."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "cross_validate(slop_one, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_validate(weighted_slop_one, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With cross-validation :\n",
    "- RMSE SlopOne : 0.8998 (mean of 5 cv)\n",
    "- RMSE WeightedSlopOne : 0.8832 (mean of 5 cv)\n",
    "\n",
    "The improvement is about 1,84 % with the weighted version of algorithm."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recommendations\n",
    "\n",
    "Let's test our reco engine on some random user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pivot_df = df_rating.pivot_table(index='userId',columns='movieId',values='rating').fillna(0)\n",
    "\n",
    "raw_user_id = 4\n",
    "# .loc is used to retrieve labels instead of index (see: https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different)\n",
    "user_ratings = pivot_df.loc[raw_user_id,:]\n",
    "# only the items the user did rate, so we can exclude them later from the recommended movies\n",
    "user_ratings = user_ratings[user_ratings != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pivot_df.columns.to_numpy() is the list of all movies IDs\n",
    "predictions = weighted_slop_one.one_user(raw_user_id, pivot_df.columns.to_numpy())\n",
    "# exclude the movies he.she has already rated\n",
    "predictions = [(iid, est) for (_,iid,_,est,_) in predictions if iid not in user_ratings]\n",
    "\n",
    "# grab the top 10 predictions\n",
    "predictions = sorted(predictions, key=lambda p: p[1], reverse=True)\n",
    "predictions = predictions[:10]\n",
    "\n",
    "# match the movie titles and show results\n",
    "df_movies[df_movies['movieId'].isin([iid for (iid, _) in predictions])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see that the recommendations are pretty consistents : same genres are found in several recommended movies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mathematics\n",
    "\n",
    "L'algo qu'on va implémenter, pour TOUS les ratings :\n",
    "\n",
    "![](medias/slop_one_with_weight.png)\n",
    "\n",
    "Cette formule permet de calculer la note prédite pour UN utilisateur sur UN film.\n",
    "\n",
    "- $u$ est un tableau incomplet comprenant les notes de l'utilisateur $U$. Incomplet, car un utilisateur ne note pas tous les items.\n",
    "- $S(u)$ est l'ensemble des items que l'user $U$ a effectivement notés\n",
    "- $S(u)-\\{j\\}$ : l'ensemble des items notés par l'user $U$, moins le singleton formé par l'item $j$\n",
    "- $j$ est l'item dont on cherche à prédire la note pour l'utilisateur $U$\n",
    "- $u_i$ est la note laissée par l'utilisateur $U$ à l'item $i$\n",
    "- $dev_{j,i}$ est la déviation moyenne entre les notes en commun des items $i$ et $j$. Elle se calcule avec `mean(r_ui - r_uj for u in U_ij)`. C'est une constante qui doit être calculée lors de la phase de de `fit` (*training*)\n",
    "- $c_{j,i}$ : la quantité (cardinalité d'un ensemble) d'utilisateurs ayant noté à la fois l'item `i` et l'item `j`\n",
    "- $\\sum{i \\in S(u)-\\{j\\}}$ : boucler sur l'ensemble des items $i$ de l'évaluation $u$, en excluant l'item $j$\n",
    "\n",
    "\n",
    "Le calcul de la déviation entre les items i et j se fait avec :\n",
    "\n",
    "\n",
    "![](medias/slop_one_deviation.png)\n",
    "\n",
    "- $u \\in S_{j,i}(X)$ : l'ensemble des évaluations $u$ contenant à la fois les items $u_j$ et $u_i$\n",
    "- Il s'agit donc de boucler sur cet ensemble, et de calculer la déviation entre $u_j$ et $u_i$ à partir des notes laissées par les utilisateurs ayant noté les deux items en même temps\n",
    "\n",
    "> À partir de cette formule, et en guise d'étape d'entraînement, on bouclera sur tous les n-uplet $(j,i)$ pour calculer leurs déviations respectives.\n",
    "> On stockera la matrice symétrique ainsi obtenue, afin de ne pas avoir à la recalculer par la suite\n",
    "> Il ne nous restera plus qu'à aller piocher dedans pour prédire la note de $j$ (cf. première formule)\n",
    "\n",
    "Comme on le voit, cet algorithme prend en considération les informations des autres utilisateurs qui ont noté le même item (comme une recherche KNN user-user basée sur le cosinus), mais également les informations provenant les autres items notés par l'utilisateur. C'est cela qui en fait un algorithme riche et efficace (?).\n",
    "\n",
    "Le `SlopeOne` du package `suprise` implémente par défaut la version non pondérée de l'algorithme :\n",
    "\n",
    "![image](medias/slop_one_without_weight.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}